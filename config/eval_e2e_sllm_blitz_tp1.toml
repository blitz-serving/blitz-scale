# If you want to generate configurations and run test cases, under workspace root directory:
# 
# ```bash
# python scripts/batchv2/main.py -t config/test_distserve.toml
# ```
#
# This will create a directory to store logs and uncommitted files.
# 
#
# To continue interrupted jobs, 
# ```bash
# python scripts/batchv2/main.py -c log_home/20250428-2036
# ```
# 
# This will read checkpoint files and skip finished jobs.

[global]
num_gpus_per_node = 4
cuda_devices = [0, 1, 2, 3, 4, 5, 6, 7]

# Only three fields available in [selection]:
# - models
# - features
# - datasets
[selection]

[[selection.tuples]]
model = "llama3_8b"
feature = "e2e_sllm"
dataset = "AzureCode2023-5min"

[[selection.tuples]]
model = "llama3_8b"
feature = "e2e_blitz"
dataset = "AzureCode2023-5min"

[[selection.tuples]]
model = "llama3_8b"
feature = "e2e_all_cache"
dataset = "AzureCode2023-5min"

# [[selection.tuples]]
# model = "llama2_7b"
# feature = "e2e_sllm"
# dataset = "AzureConv2023-5min"

# [[selection.tuples]]
# model = "llama2_7b"
# feature = "e2e_blitz"
# dataset = "AzureConv2023-5min"

# [[selection.tuples]]
# model = "llama2_7b"
# feature = "e2e_all_cache"
# dataset = "AzureConv2023-5min"

[server]
ibv_rate = 100
inter_node = true

[server.config]
g0001 = "10.254.0.10"
g0002 = "10.254.0.9"

# Fields in [router] field could be a value, a list of values, or a list of kv-pairs
# - Value: copied directly to generated instance.toml (Like port = 11236)
# - A list of values: unzip the list and generate n * m instance.toml files if there are two lists containing n and m values.
# - A list of kv-pairs: unzip the list, and use each kv-pair to update [router]'s specific field. (See [[router.tuples]] for more information)
[router]
port = 11236
prefill_lower_bound = 0.5
prefill_upper_bound = 0.7
decode_lower_bound = 0.75
decode_upper_bound = 0.95
migration_lower_bound = 0.4
migration_upper_bound = 0.8
# TODO
scale_down_threshold_millis = 333

max_prefill_num = 14
min_prefill_num = 2
max_decode_num = 14
min_decode_num = 2

mock_load_millis = 0
mock_transfer_millis = 0

# Extra envs when launching server, router and client.
# CUDA_VISIBLE_DEVICES is set in [global] section and will be ignored if set here.
[extra-envs]
LOG_LEVEL = "INFO"

[features]
sllm_cache_replace = "ngrok,impl_sllm,cache_replace"
sllm_optimal = "ngrok,impl_sllm,cache_all_hit,mutate"
blitz_ultra = "ngrok,impl_blitz,impl_live_pro,impl_fast_pro,mutate"
blitz_tanz_debug = "ngrok,impl_blitz,impl_fast_pro"
blitz_live_tanz_debug = "ngrok,impl_blitz,impl_live_pro,impl_fast_pro"

# e2e evaluation
e2e_sllm = "ngrok,impl_sllm,cache_replace"                 # sllm_cache_replace
e2e_all_cache = "ngrok,impl_sllm,cache_all_hit,mutate"     # sllm_optimal
e2e_blitz = "ngrok,impl_blitz,impl_live_pro,impl_fast_pro" # blitz_ultra
e2e_ds = "ngrok,impl_sllm,cache_replace"                   # fake

[datasets]

[datasets.AzureConv2023-5min]
dataset_path = "./dataset_home/AzureConv2023-5min.csv"
time_in_secs = 300
scale_factor = 1.0

[datasets.AzureCode2023-5min]
dataset_path = "./dataset_home/AzureCode2023-5min.csv"
time_in_secs = 360
scale_factor = 1.0

[datasets.AzureCode2023-90sec]
dataset_path = "./dataset_home/AzureCode2023-90sec.csv"
time_in_secs = 90
scale_factor = 1.1

[datasets.uniform]
dataset_path = "./dataset_home/uniform.csv"
time_in_secs = 30
scale_factor = 2.0

[models]

[models.llama2_7b]
model_path = "/nvme/blitz/model/Llama-2-7b-hf"
tokenizer = "/nvme/blitz/model/Llama-2-7b-hf/tokenizer.json"
tokens_prefilled_per_sec = 13000
tokens_transferred_per_sec = 24000
num_hidden_layers = 32
num_available_blocks = 8000
tp_size = 1
parameter_size = 14.0

[models.llama3_8b]
model_path = "/blitz_home/models/DeepSeek-R1-Distill-Llama-8B"
tokenizer = "/blitz_home/models/DeepSeek-R1-Distill-Llama-8B/tokenizer.json"
tokens_prefilled_per_sec = 15000
tokens_transferred_per_sec = 96000
num_hidden_layers = 32
num_available_blocks = 30000
tp_size = 1
parameter_size = 16.0

[models.mistral_24b]
model_path = "/blitz_home/models/Mistral-Small-24B-Instruct-2501"
tokenizer = "/blitz_home/models/Mistral-Small-24B-Instruct-2501/tokenizer.json"
tokens_prefilled_per_sec = 12000
tokens_transferred_per_sec = 160000
num_hidden_layers = 40
num_available_blocks = 42000
tp_size = 2
parameter_size = 48.0

# [models.qwen_72b]
# model_path = "/nvme/huggingface/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31"
# tokenizer = "/nvme/huggingface/models/models--Qwen--Qwen2.5-72B-Instruct/snapshots/495f39366efef23836d0cfae4fbe635880d2be31/tokenizer.json"
# tokens_prefilled_per_sec = 5300
# tokens_transferred_per_sec = 160000
# num_hidden_layers = 80
# num_available_blocks = 16000
# tp_size = 4
# parameter_size = 576.0
